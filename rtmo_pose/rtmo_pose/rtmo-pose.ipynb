{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":548400,"sourceType":"modelInstanceVersion","modelInstanceId":419102,"modelId":436765},{"sourceId":549479,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":419742,"modelId":437372}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch torchvision pillow requests numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T04:47:28.230684Z","iopub.execute_input":"2025-08-28T04:47:28.230877Z","iopub.status.idle":"2025-08-28T04:48:49.940610Z","shell.execute_reply.started":"2025-08-28T04:47:28.230860Z","shell.execute_reply":"2025-08-28T04:48:49.939851Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install rtmlib opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T04:51:25.306143Z","iopub.execute_input":"2025-08-28T04:51:25.306850Z","iopub.status.idle":"2025-08-28T04:51:28.880764Z","shell.execute_reply.started":"2025-08-28T04:51:25.306791Z","shell.execute_reply":"2025-08-28T04:51:28.879857Z"}},"outputs":[{"name":"stdout","text":"Collecting rtmlib\n  Downloading rtmlib-0.0.13-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading rtmlib-0.0.13-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rtmlib\nSuccessfully installed rtmlib-0.0.13\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install onnxruntime-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T04:52:04.176685Z","iopub.execute_input":"2025-08-28T04:52:04.177045Z","iopub.status.idle":"2025-08-28T04:52:16.809289Z","shell.execute_reply.started":"2025-08-28T04:52:04.177022Z","shell.execute_reply":"2025-08-28T04:52:16.808245Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime-gpu\n  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting coloredlogs (from onnxruntime-gpu)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime-gpu) (2.4.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime-gpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime-gpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime-gpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime-gpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime-gpu) (2024.2.0)\nDownloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.22.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### **For Images**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom rtmlib import Body, draw_skeleton\n\n# Configuration\nINPUT_DIR = '/kaggle/input/demo/other/default/1'  # Directory containing input images\nOUTPUT_DIR = \"/kaggle/working/output_results\"  # Directory to save results\ndevice = 'cuda'  # 'cpu', 'cuda', or 'mps'\nbackend = 'onnxruntime'  # 'opencv', 'onnxruntime', 'openvino'\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"images\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"data\"), exist_ok=True)\n\n# Load RTMO model using Body solution with balanced mode (includes RTMO models)\nprint(\"Loading pose estimation model...\")\nbody_estimator = Body(\n    mode='balanced',  # 'performance', 'lightweight', 'balanced' (balanced includes RTMO)\n    backend=backend,\n    device=device\n)\n\n# Define COCO keypoint labels (17 keypoints)\nkeypoint_labels = [\n    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n]\n\n# Define thresholds\npoint_threshold = 0.3\nbox_threshold = 0.3\n\ndef process_image(image_path):\n    \"\"\"Process a single image and return pose estimation results\"\"\"\n    print(f\"Processing: {image_path}\")\n    \n    # Read image\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Error: Could not load image {image_path}\")\n        return None\n    \n    # Run inference\n    keypoints, scores = body_estimator(image)\n    \n    # Extract bounding boxes and keypoints in the format similar to original JS code\n    predicted_boxes = []\n    predicted_points = []\n    \n    # rtmlib returns keypoints and scores for each detected person\n    for person_idx in range(len(keypoints)):\n        person_keypoints = keypoints[person_idx]  # Shape: [17, 2] - x, y coordinates\n        person_scores = scores[person_idx]        # Shape: [17] - confidence scores\n        \n        # Create bounding box from keypoints (find min/max x,y of visible keypoints)\n        visible_points = person_keypoints[person_scores > 0.1]  # Only use visible keypoints\n        if len(visible_points) > 0:\n            xmin = float(np.min(visible_points[:, 0]))\n            ymin = float(np.min(visible_points[:, 1]))\n            xmax = float(np.max(visible_points[:, 0]))\n            ymax = float(np.max(visible_points[:, 1]))\n            \n            # Calculate average score as box score\n            box_score = float(np.mean(person_scores[person_scores > 0.1]))\n            \n            predicted_boxes.append([xmin, ymin, xmax, ymax, box_score])\n            \n            # Convert keypoints to format [x, y, score]\n            person_points = []\n            for j in range(len(person_keypoints)):\n                x, y = person_keypoints[j]\n                score = person_scores[j]\n                person_points.append([float(x), float(y), float(score)])\n            \n            predicted_points.append(person_points)\n    \n    return {\n        'image_shape': image.shape,\n        'predicted_boxes': predicted_boxes,\n        'predicted_points': predicted_points,\n        'raw_keypoints': keypoints.tolist() if isinstance(keypoints, np.ndarray) else keypoints,\n        'raw_scores': scores.tolist() if isinstance(scores, np.ndarray) else scores\n    }\n\ndef save_results(image_path, results, filename_base):\n    \"\"\"Save pose estimation results to files\"\"\"\n    \n    # Save detection data as JSON\n    json_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_pose_data.json\")\n    with open(json_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    # Save readable text output\n    txt_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_pose_results.txt\")\n    with open(txt_path, 'w') as f:\n        f.write(f\"Pose Estimation Results for: {os.path.basename(image_path)}\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n        \n        # Display results\n        for i in range(len(results['predicted_boxes'])):\n            if len(results['predicted_boxes'][i]) == 5:\n                xmin, ymin, xmax, ymax, box_score = results['predicted_boxes'][i]\n            else:\n                continue\n            \n            if box_score < box_threshold:\n                continue\n            \n            x1 = round(xmin, 2)\n            y1 = round(ymin, 2)\n            x2 = round(xmax, 2)\n            y2 = round(ymax, 2)\n            \n            result_line = f\"Found person at [{x1}, {y1}, {x2}, {y2}] with score {box_score:.3f}\\n\"\n            print(result_line.strip())\n            f.write(result_line)\n            \n            if i < len(results['predicted_points']):\n                points = results['predicted_points'][i]  # shape [17, 3]\n                for point_id in range(len(points)):\n                    if point_id < len(keypoint_labels):\n                        label = keypoint_labels[point_id]\n                    else:\n                        label = f\"point_{point_id}\"\n                    \n                    if len(points[point_id]) >= 3:\n                        x, y, point_score = points[point_id][:3]\n                        if point_score < point_threshold:\n                            continue\n                        \n                        point_line = f\"  - {label}: ({round(x, 2)}, {round(y, 2)}) with score {point_score:.3f}\\n\"\n                        print(point_line.strip())\n                        f.write(point_line)\n                f.write(\"\\n\")\n    \n    # Create and save visualization\n    image = cv2.imread(image_path)\n    keypoints_array = np.array(results['raw_keypoints'])\n    scores_array = np.array(results['raw_scores'])\n    \n    # Draw skeleton on image\n    img_with_pose = draw_skeleton(image, keypoints_array, scores_array, kpt_thr=0.3)\n    \n    # Save the visualization\n    output_image_path = os.path.join(OUTPUT_DIR, \"images\", f\"{filename_base}_pose_estimation.jpg\")\n    cv2.imwrite(output_image_path, img_with_pose)\n    \n    print(f\"Results saved:\")\n    print(f\"  - JSON data: {json_path}\")\n    print(f\"  - Text results: {txt_path}\")\n    print(f\"  - Visualization: {output_image_path}\")\n\ndef main():\n    \"\"\"Main function to process all images in input directory\"\"\"\n    \n    # Check if input directory exists\n    if not os.path.exists(INPUT_DIR):\n        print(f\"Creating input directory: {INPUT_DIR}\")\n        os.makedirs(INPUT_DIR)\n        print(f\"Please place your images in the '{INPUT_DIR}' directory and run again.\")\n        return\n    \n    # Supported image extensions\n    supported_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n    \n    # Get list of image files\n    image_files = []\n    for file in os.listdir(INPUT_DIR):\n        if any(file.lower().endswith(ext) for ext in supported_extensions):\n            image_files.append(file)\n    \n    if not image_files:\n        print(f\"No supported image files found in '{INPUT_DIR}' directory.\")\n        print(f\"Supported formats: {', '.join(supported_extensions)}\")\n        return\n    \n    print(f\"Found {len(image_files)} image(s) to process:\")\n    for img_file in image_files:\n        print(f\"  - {img_file}\")\n    print()\n    \n    # Process each image\n    for image_file in image_files:\n        image_path = os.path.join(INPUT_DIR, image_file)\n        filename_base = os.path.splitext(image_file)[0]\n        \n        try:\n            # Process the image\n            results = process_image(image_path)\n            \n            if results is not None:\n                # Save results\n                save_results(image_path, results, filename_base)\n                print(\"-\" * 60)\n            else:\n                print(f\"Failed to process {image_file}\")\n                \n        except Exception as e:\n            print(f\"Error processing {image_file}: {str(e)}\")\n            continue\n    \n    print(\"\\nProcessing complete!\")\n    print(f\"Check the '{OUTPUT_DIR}' directory for results:\")\n    print(f\"  - '{OUTPUT_DIR}/images/' contains pose estimation visualizations\")\n    print(f\"  - '{OUTPUT_DIR}/data/' contains JSON data and text results\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:01:13.824842Z","iopub.execute_input":"2025-08-28T05:01:13.825217Z","iopub.status.idle":"2025-08-28T05:01:14.738231Z","shell.execute_reply.started":"2025-08-28T05:01:13.825188Z","shell.execute_reply":"2025-08-28T05:01:14.737614Z"}},"outputs":[{"name":"stdout","text":"Loading pose estimation model...\nload /root/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0;93m2025-08-28 05:01:13.943517556 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:01:13.943545899 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n\u001b[0;93m2025-08-28 05:01:14.037578218 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:01:14.037601458 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"load /root/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.onnx with onnxruntime backend\nFound 1 image(s) to process:\n  - demo.jpg\n\nProcessing: /kaggle/input/demo/other/default/1/demo.jpg\nFound person at [661.52, 556.54, 1940.59, 2656.29] with score 0.975\n- nose: (1334.33, 793.13) with score 0.978\n- left_eye: (1386.08, 756.17) with score 0.988\n- right_eye: (1304.75, 756.17) with score 1.048\n- left_ear: (1452.62, 807.92) with score 0.944\n- right_ear: (1260.39, 800.53) with score 0.966\n- left_shoulder: (1556.13, 1000.15) with score 0.900\n- right_shoulder: (1134.7, 992.76) with score 0.933\n- left_elbow: (1829.69, 807.92) with score 1.027\n- right_elbow: (853.75, 830.1) with score 0.995\n- left_wrist: (1940.59, 556.54) with score 1.055\n- right_wrist: (661.52, 571.33) with score 1.060\n- left_hip: (1460.02, 1672.96) with score 0.871\n- right_hip: (1179.06, 1672.96) with score 0.887\n- left_knee: (1622.67, 2138.74) with score 0.979\n- right_knee: (1031.19, 2160.92) with score 0.967\n- left_ankle: (1755.75, 2626.71) with score 0.975\n- right_ankle: (912.9, 2656.29) with score 0.993\nFound person at [2710.49, 660.54, 4019.68, 2668.46] with score 0.960\n- nose: (3372.44, 815.0) with score 0.922\n- left_eye: (3394.5, 778.22) with score 0.942\n- right_eye: (3313.6, 785.58) with score 0.957\n- left_ear: (3423.92, 837.06) with score 0.882\n- right_ear: (3240.05, 851.77) with score 0.939\n- left_shoulder: (3556.31, 1020.94) with score 0.946\n- right_shoulder: (3100.3, 1050.36) with score 0.938\n- left_elbow: (3857.87, 947.39) with score 0.970\n- right_elbow: (2820.81, 954.74) with score 1.021\n- left_wrist: (4019.68, 689.96) with score 1.025\n- right_wrist: (2710.49, 660.54) with score 1.084\n- left_hip: (3504.83, 1734.37) with score 0.888\n- right_hip: (3240.05, 1734.37) with score 0.883\n- left_knee: (3644.57, 2219.8) with score 0.959\n- right_knee: (3092.95, 2219.8) with score 0.935\n- left_ankle: (3696.06, 2668.46) with score 1.017\n- right_ankle: (2982.62, 2668.46) with score 1.016\nResults saved:\n  - JSON data: /kaggle/working/output_results/data/demo_pose_data.json\n  - Text results: /kaggle/working/output_results/data/demo_pose_results.txt\n  - Visualization: /kaggle/working/output_results/images/demo_pose_estimation.jpg\n------------------------------------------------------------\n\nProcessing complete!\nCheck the '/kaggle/working/output_results' directory for results:\n  - '/kaggle/working/output_results/images/' contains pose estimation visualizations\n  - '/kaggle/working/output_results/data/' contains JSON data and text results\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### **For Videos**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom rtmlib import Body, draw_skeleton\n\n# Configuration\nINPUT_DIR = '/kaggle/input/rtmo/other/default/1'  # Directory containing input videos\nOUTPUT_DIR = \"/kaggle/working/video_output\"  # Directory to save results\ndevice = 'cuda'  # 'cpu', 'cuda', or 'mps'\nbackend = 'onnxruntime'  # 'opencv', 'onnxruntime', 'openvino'\n\n# Video processing settings\nSAVE_EVERY_N_FRAMES = 10  # Save detailed results every N frames (to avoid too many files)\nFRAME_SKIP = 1  # Process every N frames (1 = process all frames, 2 = every other frame)\nMAX_FRAMES = None  # Maximum frames to process (None = process entire video)\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"videos\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"frames\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"data\"), exist_ok=True)\n\n# Load RTMO model using Body solution with balanced mode (includes RTMO models)\nprint(\"Loading pose estimation model...\")\nbody_estimator = Body(\n    mode='balanced',  # 'performance', 'lightweight', 'balanced' (balanced includes RTMO)\n    backend=backend,\n    device=device\n)\n\n# Define COCO keypoint labels (17 keypoints)\nkeypoint_labels = [\n    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n]\n\n# Define thresholds\npoint_threshold = 0.3\nbox_threshold = 0.3\n\ndef process_frame(frame):\n    \"\"\"Process a single frame and return pose estimation results\"\"\"\n    \n    # Run inference\n    keypoints, scores = body_estimator(frame)\n    \n    # Extract bounding boxes and keypoints in the format similar to original JS code\n    predicted_boxes = []\n    predicted_points = []\n    \n    # rtmlib returns keypoints and scores for each detected person\n    for person_idx in range(len(keypoints)):\n        person_keypoints = keypoints[person_idx]  # Shape: [17, 2] - x, y coordinates\n        person_scores = scores[person_idx]        # Shape: [17] - confidence scores\n        \n        # Create bounding box from keypoints (find min/max x,y of visible keypoints)\n        visible_points = person_keypoints[person_scores > 0.1]  # Only use visible keypoints\n        if len(visible_points) > 0:\n            xmin = float(np.min(visible_points[:, 0]))\n            ymin = float(np.min(visible_points[:, 1]))\n            xmax = float(np.max(visible_points[:, 0]))\n            ymax = float(np.max(visible_points[:, 1]))\n            \n            # Calculate average score as box score\n            box_score = float(np.mean(person_scores[person_scores > 0.1]))\n            \n            predicted_boxes.append([xmin, ymin, xmax, ymax, box_score])\n            \n            # Convert keypoints to format [x, y, score]\n            person_points = []\n            for j in range(len(person_keypoints)):\n                x, y = person_keypoints[j]\n                score = person_scores[j]\n                person_points.append([float(x), float(y), float(score)])\n            \n            predicted_points.append(person_points)\n    \n    return {\n        'frame_shape': frame.shape,\n        'predicted_boxes': predicted_boxes,\n        'predicted_points': predicted_points,\n        'raw_keypoints': keypoints.tolist() if isinstance(keypoints, np.ndarray) else keypoints,\n        'raw_scores': scores.tolist() if isinstance(scores, np.ndarray) else scores\n    }\n\ndef save_frame_results(frame_results, filename_base, frame_number):\n    \"\"\"Save pose estimation results for a specific frame\"\"\"\n    \n    # Save detection data as JSON\n    json_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_frame_{frame_number:06d}_pose_data.json\")\n    with open(json_path, 'w') as f:\n        json.dump(frame_results, f, indent=2)\n    \n    # Save readable text output\n    txt_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_frame_{frame_number:06d}_pose_results.txt\")\n    with open(txt_path, 'w') as f:\n        f.write(f\"Pose Estimation Results for Frame {frame_number}\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n        \n        # Display results\n        for i in range(len(frame_results['predicted_boxes'])):\n            if len(frame_results['predicted_boxes'][i]) == 5:\n                xmin, ymin, xmax, ymax, box_score = frame_results['predicted_boxes'][i]\n            else:\n                continue\n            \n            if box_score < box_threshold:\n                continue\n            \n            x1 = round(xmin, 2)\n            y1 = round(ymin, 2)\n            x2 = round(xmax, 2)\n            y2 = round(ymax, 2)\n            \n            result_line = f\"Found person at [{x1}, {y1}, {x2}, {y2}] with score {box_score:.3f}\\n\"\n            f.write(result_line)\n            \n            if i < len(frame_results['predicted_points']):\n                points = frame_results['predicted_points'][i]  # shape [17, 3]\n                for point_id in range(len(points)):\n                    if point_id < len(keypoint_labels):\n                        label = keypoint_labels[point_id]\n                    else:\n                        label = f\"point_{point_id}\"\n                    \n                    if len(points[point_id]) >= 3:\n                        x, y, point_score = points[point_id][:3]\n                        if point_score < point_threshold:\n                            continue\n                        \n                        point_line = f\"  - {label}: ({round(x, 2)}, {round(y, 2)}) with score {point_score:.3f}\\n\"\n                        f.write(point_line)\n                f.write(\"\\n\")\n\ndef process_video(video_path):\n    \"\"\"Process a video file and return pose estimation results for all frames\"\"\"\n    print(f\"Processing video: {video_path}\")\n    \n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video {video_path}\")\n        return None\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    print(f\"Video info: {width}x{height}, {fps} FPS, {total_frames} frames\")\n    \n    # Prepare output video writer\n    filename_base = os.path.splitext(os.path.basename(video_path))[0]\n    output_video_path = os.path.join(OUTPUT_DIR, \"videos\", f\"{filename_base}_pose_estimation.mp4\")\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n    \n    # Store all frame results\n    all_frame_results = []\n    frame_number = 0\n    processed_frames = 0\n    \n    print(\"Processing frames...\")\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Skip frames if needed\n        if frame_number % FRAME_SKIP != 0:\n            frame_number += 1\n            continue\n        \n        # Process frame\n        try:\n            frame_results = process_frame(frame)\n            \n            if frame_results is not None:\n                # Add frame number to results\n                frame_results['frame_number'] = frame_number\n                frame_results['timestamp'] = frame_number / fps\n                all_frame_results.append(frame_results)\n                \n                # Create visualization\n                keypoints_array = np.array(frame_results['raw_keypoints'])\n                scores_array = np.array(frame_results['raw_scores'])\n                \n                # Draw skeleton on frame\n                frame_with_pose = draw_skeleton(frame.copy(), keypoints_array, scores_array, kpt_thr=0.3)\n                \n                # Write frame to output video\n                out.write(frame_with_pose)\n                \n                # Save detailed results for every Nth frame\n                if frame_number % SAVE_EVERY_N_FRAMES == 0:\n                    save_frame_results(frame_results, filename_base, frame_number)\n                    \n                    # Save frame image\n                    frame_image_path = os.path.join(OUTPUT_DIR, \"frames\", f\"{filename_base}_frame_{frame_number:06d}.jpg\")\n                    cv2.imwrite(frame_image_path, frame_with_pose)\n                \n                processed_frames += 1\n                \n                # Print progress\n                if frame_number % (fps * 5) == 0:  # Every 5 seconds\n                    progress = (frame_number / total_frames) * 100\n                    print(f\"Progress: {progress:.1f}% - Frame {frame_number}/{total_frames}\")\n                \n                # Check if we've reached max frames limit\n                if MAX_FRAMES is not None and processed_frames >= MAX_FRAMES:\n                    print(f\"Reached maximum frames limit: {MAX_FRAMES}\")\n                    break\n            \n        except Exception as e:\n            print(f\"Error processing frame {frame_number}: {str(e)}\")\n        \n        frame_number += 1\n    \n    # Release resources\n    cap.release()\n    out.release()\n    \n    # Save complete video analysis\n    video_summary = {\n        'video_path': video_path,\n        'video_info': {\n            'width': width,\n            'height': height,\n            'fps': fps,\n            'total_frames': total_frames,\n            'duration_seconds': total_frames / fps\n        },\n        'processing_info': {\n            'frames_processed': processed_frames,\n            'frame_skip': FRAME_SKIP,\n            'save_every_n_frames': SAVE_EVERY_N_FRAMES\n        },\n        'frame_results': all_frame_results\n    }\n    \n    # Save complete analysis as JSON\n    video_json_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_complete_analysis.json\")\n    with open(video_json_path, 'w') as f:\n        json.dump(video_summary, f, indent=2)\n    \n    # Save video summary as text\n    summary_txt_path = os.path.join(OUTPUT_DIR, \"data\", f\"{filename_base}_video_summary.txt\")\n    with open(summary_txt_path, 'w') as f:\n        f.write(f\"Video Pose Estimation Summary\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Video: {os.path.basename(video_path)}\\n\")\n        f.write(f\"Resolution: {width}x{height}\\n\")\n        f.write(f\"FPS: {fps}\\n\")\n        f.write(f\"Duration: {total_frames/fps:.2f} seconds\\n\")\n        f.write(f\"Total Frames: {total_frames}\\n\")\n        f.write(f\"Processed Frames: {processed_frames}\\n\")\n        f.write(f\"Frame Skip: {FRAME_SKIP}\\n\\n\")\n        \n        # Statistics\n        total_persons = sum(len(frame['predicted_boxes']) for frame in all_frame_results)\n        avg_persons_per_frame = total_persons / len(all_frame_results) if all_frame_results else 0\n        \n        f.write(f\"Statistics:\\n\")\n        f.write(f\"  - Total person detections: {total_persons}\\n\")\n        f.write(f\"  - Average persons per frame: {avg_persons_per_frame:.2f}\\n\")\n        f.write(f\"  - Frames with detections: {len([f for f in all_frame_results if len(f['predicted_boxes']) > 0])}\\n\")\n    \n    return {\n        'output_video': output_video_path,\n        'summary_json': video_json_path,\n        'summary_txt': summary_txt_path,\n        'processed_frames': processed_frames\n    }\n\ndef main():\n    \"\"\"Main function to process all videos in input directory\"\"\"\n    \n    # Check if input directory exists\n    if not os.path.exists(INPUT_DIR):\n        print(f\"Creating input directory: {INPUT_DIR}\")\n        os.makedirs(INPUT_DIR)\n        print(f\"Please place your videos in the '{INPUT_DIR}' directory and run again.\")\n        return\n    \n    # Supported video extensions\n    supported_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v'}\n    \n    # Get list of video files\n    video_files = []\n    for file in os.listdir(INPUT_DIR):\n        if any(file.lower().endswith(ext) for ext in supported_extensions):\n            video_files.append(file)\n    \n    if not video_files:\n        print(f\"No supported video files found in '{INPUT_DIR}' directory.\")\n        print(f\"Supported formats: {', '.join(supported_extensions)}\")\n        return\n    \n    print(f\"Found {len(video_files)} video(s) to process:\")\n    for vid_file in video_files:\n        print(f\"  - {vid_file}\")\n    print()\n    \n    print(f\"Processing settings:\")\n    print(f\"  - Device: {device}\")\n    print(f\"  - Backend: {backend}\")\n    print(f\"  - Frame skip: {FRAME_SKIP} (process every {FRAME_SKIP} frame(s))\")\n    print(f\"  - Save detailed results every: {SAVE_EVERY_N_FRAMES} frames\")\n    print(f\"  - Max frames: {MAX_FRAMES if MAX_FRAMES else 'unlimited'}\")\n    print()\n    \n    # Process each video\n    for video_file in video_files:\n        video_path = os.path.join(INPUT_DIR, video_file)\n        filename_base = os.path.splitext(video_file)[0]\n        \n        try:\n            print(f\"{'='*60}\")\n            print(f\"Processing: {video_file}\")\n            print(f\"{'='*60}\")\n            \n            # Process the video\n            results = process_video(video_path)\n            \n            if results is not None:\n                print(f\"\\nâœ… Video processing complete!\")\n                print(f\"Results saved:\")\n                print(f\"  - Output video: {results['output_video']}\")\n                print(f\"  - Complete analysis: {results['summary_json']}\")\n                print(f\"  - Text summary: {results['summary_txt']}\")\n                print(f\"  - Processed {results['processed_frames']} frames\")\n                print(f\"  - Frame images saved to: {OUTPUT_DIR}/frames/\")\n                print()\n            else:\n                print(f\"âŒ Failed to process {video_file}\")\n                \n        except Exception as e:\n            print(f\"âŒ Error processing {video_file}: {str(e)}\")\n            continue\n    \n    print(\"\\nğŸ‰ All videos processed!\")\n    print(f\"Check the '{OUTPUT_DIR}' directory for results:\")\n    print(f\"  - '{OUTPUT_DIR}/videos/' contains pose estimation videos\")\n    print(f\"  - '{OUTPUT_DIR}/frames/' contains sample frame images with poses\")\n    print(f\"  - '{OUTPUT_DIR}/data/' contains JSON data and text summaries\")\n\ndef print_frame_results(frame_results, frame_number):\n    \"\"\"Print pose estimation results for a frame\"\"\"\n    print(f\"\\nFrame {frame_number} results:\")\n    \n    for i in range(len(frame_results['predicted_boxes'])):\n        if len(frame_results['predicted_boxes'][i]) == 5:\n            xmin, ymin, xmax, ymax, box_score = frame_results['predicted_boxes'][i]\n        else:\n            continue\n        \n        if box_score < box_threshold:\n            continue\n        \n        x1 = round(xmin, 2)\n        y1 = round(ymin, 2)\n        x2 = round(xmax, 2)\n        y2 = round(ymax, 2)\n        \n        print(f\"  Found person at [{x1}, {y1}, {x2}, {y2}] with score {box_score:.3f}\")\n        \n        if i < len(frame_results['predicted_points']):\n            points = frame_results['predicted_points'][i]  # shape [17, 3]\n            visible_keypoints = []\n            for point_id in range(len(points)):\n                if point_id < len(keypoint_labels):\n                    label = keypoint_labels[point_id]\n                else:\n                    label = f\"point_{point_id}\"\n                \n                if len(points[point_id]) >= 3:\n                    x, y, point_score = points[point_id][:3]\n                    if point_score >= point_threshold:\n                        visible_keypoints.append(f\"{label}: ({round(x, 2)}, {round(y, 2)}) score={point_score:.3f}\")\n            \n            # Only show first few keypoints to avoid cluttering console\n            print(f\"    Key points: {len(visible_keypoints)} visible\")\n            for kpt in visible_keypoints[:3]:  # Show first 3 keypoints\n                print(f\"      - {kpt}\")\n            if len(visible_keypoints) > 3:\n                print(f\"      ... and {len(visible_keypoints)-3} more\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:07:42.498839Z","iopub.execute_input":"2025-08-28T05:07:42.499101Z","iopub.status.idle":"2025-08-28T05:08:04.656472Z","shell.execute_reply.started":"2025-08-28T05:07:42.499084Z","shell.execute_reply":"2025-08-28T05:08:04.655766Z"}},"outputs":[{"name":"stdout","text":"Loading pose estimation model...\nload /root/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\nload /root/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.onnx with onnxruntime backend\nFound 1 video(s) to process:\n  - test.mp4\n\nProcessing settings:\n  - Device: cuda\n  - Backend: onnxruntime\n  - Frame skip: 1 (process every 1 frame(s))\n  - Save detailed results every: 10 frames\n  - Max frames: unlimited\n\n============================================================\nProcessing: test.mp4\n============================================================\nProcessing video: /kaggle/input/rtmo/other/default/1/test.mp4\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0;93m2025-08-28 05:07:42.582590849 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:07:42.582620602 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n\u001b[0;93m2025-08-28 05:07:42.663185946 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:07:42.663211917 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"Video info: 1920x1080, 25 FPS, 315 frames\nProcessing frames...\nProgress: 0.0% - Frame 0/315\nProgress: 39.7% - Frame 125/315\nProgress: 79.4% - Frame 250/315\n\nâœ… Video processing complete!\nResults saved:\n  - Output video: /kaggle/working/video_output/videos/test_pose_estimation.mp4\n  - Complete analysis: /kaggle/working/video_output/data/test_complete_analysis.json\n  - Text summary: /kaggle/working/video_output/data/test_video_summary.txt\n  - Processed 315 frames\n  - Frame images saved to: /kaggle/working/video_output/frames/\n\n\nğŸ‰ All videos processed!\nCheck the '/kaggle/working/video_output' directory for results:\n  - '/kaggle/working/video_output/videos/' contains pose estimation videos\n  - '/kaggle/working/video_output/frames/' contains sample frame images with poses\n  - '/kaggle/working/video_output/data/' contains JSON data and text summaries\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### **With .npz file**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom rtmlib import Body, draw_skeleton\nimport shutil\nfrom datetime import datetime\n\n# Configuration\nINPUT_DIR = '/kaggle/input/rtmo/other/default/1'  # Directory containing input videos\nBASE_OUTPUT_DIR = \"/kaggle/working/pose_estimation_results\"  # Base directory to save all results\ndevice = 'cuda'  # 'cpu', 'cuda', or 'mps'\nbackend = 'onnxruntime'  # 'opencv', 'onnxruntime', 'openvino'\n\n# Video processing settings\nFRAME_SKIP = 1  # Process every N frames (1 = process all frames, 2 = every other frame)\nMAX_FRAMES = None  # Maximum frames to process (None = process entire video)\n\n# Create organized directory structure\ndef create_project_structure():\n    \"\"\"Create organized folder structure for the pose estimation project\"\"\"\n    \n    # Main project directories\n    dirs_to_create = [\n        BASE_OUTPUT_DIR,\n        os.path.join(BASE_OUTPUT_DIR, \"input_videos\"),\n        os.path.join(BASE_OUTPUT_DIR, \"output_videos\"),\n        os.path.join(BASE_OUTPUT_DIR, \"pose_data_npz\")\n    ]\n    \n    for dir_path in dirs_to_create:\n        os.makedirs(dir_path, exist_ok=True)\n    \n    return {\n        'input_videos': os.path.join(BASE_OUTPUT_DIR, \"input_videos\"),\n        'output_videos': os.path.join(BASE_OUTPUT_DIR, \"output_videos\"),\n        'pose_data_npz': os.path.join(BASE_OUTPUT_DIR, \"pose_data_npz\")\n    }\n\n# Create directory structure\noutput_paths = create_project_structure()\n\n# Load RTMO model using Body solution with balanced mode (includes RTMO models)\nprint(\"Loading pose estimation model...\")\nbody_estimator = Body(\n    mode='balanced',  # 'performance', 'lightweight', 'balanced' (balanced includes RTMO)\n    backend=backend,\n    device=device\n)\n\n# Define COCO keypoint labels (17 keypoints)\nkeypoint_labels = [\n    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n]\n\n# Define thresholds\npoint_threshold = 0.3\nbox_threshold = 0.3\n\ndef process_frame(frame):\n    \"\"\"Process a single frame and return pose estimation results\"\"\"\n    \n    # Run inference\n    keypoints, scores = body_estimator(frame)\n    \n    # Extract bounding boxes and keypoints in the format similar to original JS code\n    predicted_boxes = []\n    predicted_points = []\n    \n    # rtmlib returns keypoints and scores for each detected person\n    for person_idx in range(len(keypoints)):\n        person_keypoints = keypoints[person_idx]  # Shape: [17, 2] - x, y coordinates\n        person_scores = scores[person_idx]        # Shape: [17] - confidence scores\n        \n        # Create bounding box from keypoints (find min/max x,y of visible keypoints)\n        visible_points = person_keypoints[person_scores > 0.1]  # Only use visible keypoints\n        if len(visible_points) > 0:\n            xmin = float(np.min(visible_points[:, 0]))\n            ymin = float(np.min(visible_points[:, 1]))\n            xmax = float(np.max(visible_points[:, 0]))\n            ymax = float(np.max(visible_points[:, 1]))\n            \n            # Calculate average score as box score\n            box_score = float(np.mean(person_scores[person_scores > 0.1]))\n            \n            predicted_boxes.append([xmin, ymin, xmax, ymax, box_score])\n            \n            # Convert keypoints to format [x, y, score]\n            person_points = []\n            for j in range(len(person_keypoints)):\n                x, y = person_keypoints[j]\n                score = person_scores[j]\n                person_points.append([float(x), float(y), float(score)])\n            \n            predicted_points.append(person_points)\n    \n    return {\n        'frame_shape': frame.shape,\n        'predicted_boxes': predicted_boxes,\n        'predicted_points': predicted_points,\n        'raw_keypoints': keypoints,\n        'raw_scores': scores\n    }\n\ndef save_pose_data_npz(all_frame_results, filename_base, video_info):\n    \"\"\"Save all pose data in NPZ format\"\"\"\n    \n    if not all_frame_results:\n        print(\"âš ï¸  No frame results to save\")\n        return None\n    \n    # Prepare data for NPZ\n    num_frames = len(all_frame_results)\n    max_persons = max(len(frame['predicted_boxes']) for frame in all_frame_results) if all_frame_results else 0\n    \n    if max_persons == 0:\n        max_persons = 1  # Ensure at least 1 person dimension\n    \n    # Frame metadata\n    frame_numbers = np.array([frame['frame_number'] for frame in all_frame_results])\n    timestamps = np.array([frame['timestamp'] for frame in all_frame_results])\n    \n    # Initialize pose data arrays\n    bboxes = np.full((num_frames, max_persons, 5), -1.0, dtype=np.float32)\n    keypoints_coords = np.full((num_frames, max_persons, 17, 2), -1.0, dtype=np.float32)\n    keypoints_scores = np.full((num_frames, max_persons, 17), -1.0, dtype=np.float32)\n    person_valid = np.zeros((num_frames, max_persons), dtype=bool)\n    \n    # Fill arrays with data\n    for frame_idx, frame_result in enumerate(all_frame_results):\n        num_persons = len(frame_result['predicted_boxes'])\n        \n        for person_idx in range(min(num_persons, max_persons)):\n            # Bounding boxes\n            if len(frame_result['predicted_boxes'][person_idx]) == 5:\n                bboxes[frame_idx, person_idx] = frame_result['predicted_boxes'][person_idx]\n                person_valid[frame_idx, person_idx] = True\n            \n            # Keypoints\n            if person_idx < len(frame_result['predicted_points']):\n                points = frame_result['predicted_points'][person_idx]\n                for kpt_idx in range(min(len(points), 17)):\n                    if len(points[kpt_idx]) >= 3:\n                        x, y, score = points[kpt_idx][:3]\n                        keypoints_coords[frame_idx, person_idx, kpt_idx] = [x, y]\n                        keypoints_scores[frame_idx, person_idx, kpt_idx] = score\n    \n    # Save NPZ file\n    npz_path = os.path.join(output_paths['pose_data_npz'], f\"{filename_base}_pose_data.npz\")\n    \n    np.savez_compressed(\n        npz_path,\n        # Time information\n        frame_numbers=frame_numbers,\n        timestamps=timestamps,\n        \n        # Pose data (separate coordinates and scores for easier analysis)\n        bboxes=bboxes,\n        keypoints_coords=keypoints_coords,\n        keypoints_scores=keypoints_scores,\n        person_valid=person_valid,\n        \n        # Metadata (stored as JSON strings)\n        video_info=video_info,\n        keypoint_labels=keypoint_labels,\n        thresholds=np.array([point_threshold, box_threshold]),\n        \n        # Processing info\n        processing_settings={\n            'frame_skip': FRAME_SKIP,\n            'device': device,\n            'backend': backend,\n            'processing_timestamp': datetime.now().isoformat()\n        }\n    )\n    \n    print(f\"ğŸ’¾ NPZ file saved: {npz_path}\")\n    print(f\"ğŸ“ Data shape: {num_frames} frames, up to {max_persons} persons, 17 keypoints each\")\n    \n    return npz_path\n\ndef copy_input_videos(video_files):\n    \"\"\"Copy input videos to the organized structure\"\"\"\n    copied_videos = []\n    \n    for video_file in video_files:\n        src_path = os.path.join(INPUT_DIR, video_file)\n        dst_path = os.path.join(output_paths['input_videos'], video_file)\n        \n        try:\n            shutil.copy2(src_path, dst_path)\n            copied_videos.append(video_file)\n            print(f\"ğŸ“ Copied input video: {video_file}\")\n        except Exception as e:\n            print(f\"âŒ Error copying {video_file}: {str(e)}\")\n    \n    return copied_videos\n\ndef process_video(video_path):\n    \"\"\"Process a video file and return pose estimation results for all frames\"\"\"\n    print(f\"ğŸ¬ Processing video: {os.path.basename(video_path)}\")\n    \n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"âŒ Error: Could not open video {video_path}\")\n        return None\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    video_info = {\n        'width': width,\n        'height': height,\n        'fps': fps,\n        'total_frames': total_frames,\n        'duration_seconds': total_frames / fps\n    }\n    \n    print(f\"ğŸ“¹ Video info: {width}x{height}, {fps} FPS, {total_frames} frames, {total_frames/fps:.2f}s\")\n    \n    # Prepare output video writer\n    filename_base = os.path.splitext(os.path.basename(video_path))[0]\n    output_video_path = os.path.join(output_paths['output_videos'], f\"{filename_base}_pose_estimation.mp4\")\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n    \n    # Store all frame results\n    all_frame_results = []\n    frame_number = 0\n    processed_frames = 0\n    \n    print(\"ğŸ”„ Processing frames...\")\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Skip frames if needed\n        if frame_number % FRAME_SKIP != 0:\n            frame_number += 1\n            continue\n        \n        # Process frame\n        try:\n            frame_results = process_frame(frame)\n            \n            if frame_results is not None:\n                # Add frame metadata\n                frame_results['frame_number'] = frame_number\n                frame_results['timestamp'] = frame_number / fps\n                all_frame_results.append(frame_results)\n                \n                # Create visualization\n                keypoints_array = np.array(frame_results['raw_keypoints'])\n                scores_array = np.array(frame_results['raw_scores'])\n                \n                # Draw skeleton on frame\n                frame_with_pose = draw_skeleton(frame.copy(), keypoints_array, scores_array, kpt_thr=0.3)\n                \n                # Write frame to output video\n                out.write(frame_with_pose)\n                \n                processed_frames += 1\n                \n                # Print progress\n                if frame_number % (fps * 5) == 0:  # Every 5 seconds\n                    progress = (frame_number / total_frames) * 100\n                    print(f\"â³ Progress: {progress:.1f}% - Frame {frame_number}/{total_frames}\")\n                \n                # Check if we've reached max frames limit\n                if MAX_FRAMES is not None and processed_frames >= MAX_FRAMES:\n                    print(f\"ğŸ›‘ Reached maximum frames limit: {MAX_FRAMES}\")\n                    break\n            \n        except Exception as e:\n            error_msg = f\"âŒ Error processing frame {frame_number}: {str(e)}\"\n            print(error_msg)\n        \n        frame_number += 1\n    \n    # Close resources\n    cap.release()\n    out.release()\n    \n    # Save pose data in NPZ format\n    print(\"ğŸ’¾ Saving pose data to NPZ format...\")\n    npz_path = save_pose_data_npz(all_frame_results, filename_base, video_info)\n    \n    return {\n        'output_video': output_video_path,\n        'npz_file': npz_path,\n        'processed_frames': processed_frames\n    }\n\ndef main():\n    \"\"\"Main function to process all videos in input directory\"\"\"\n    \n    # Check if input directory exists\n    if not os.path.exists(INPUT_DIR):\n        print(f\"ğŸ“ Creating input directory: {INPUT_DIR}\")\n        os.makedirs(INPUT_DIR)\n        print(f\"Please place your videos in the '{INPUT_DIR}' directory and run again.\")\n        return\n    \n    # Supported video extensions\n    supported_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v'}\n    \n    # Get list of video files\n    video_files = []\n    for file in os.listdir(INPUT_DIR):\n        if any(file.lower().endswith(ext) for ext in supported_extensions):\n            video_files.append(file)\n    \n    if not video_files:\n        print(f\"âŒ No supported video files found in '{INPUT_DIR}' directory.\")\n        print(f\"Supported formats: {', '.join(supported_extensions)}\")\n        return\n    \n    print(f\"ğŸ¯ Found {len(video_files)} video(s) to process:\")\n    for vid_file in video_files:\n        print(f\"  ğŸ“¹ {vid_file}\")\n    print()\n    \n    print(f\"âš™ï¸  Processing settings:\")\n    print(f\"  ğŸ–¥ï¸  Device: {device}\")\n    print(f\"  ğŸ”§ Backend: {backend}\")\n    print(f\"  â­ï¸  Frame skip: {FRAME_SKIP} (process every {FRAME_SKIP} frame(s))\")\n    print(f\"  ğŸ”¢ Max frames: {MAX_FRAMES if MAX_FRAMES else 'unlimited'}\")\n    print()\n    \n    # Copy input videos to organized structure\n    print(\"ğŸ“‹ Organizing input videos...\")\n    copied_videos = copy_input_videos(video_files)\n    \n    # Process each video\n    all_results = []\n    \n    for video_file in video_files:\n        video_path = os.path.join(INPUT_DIR, video_file)\n        filename_base = os.path.splitext(video_file)[0]\n        \n        try:\n            print(f\"\\n{'='*70}\")\n            print(f\"ğŸš€ Processing: {video_file}\")\n            print(f\"{'='*70}\")\n            \n            # Process the video\n            results = process_video(video_path)\n            \n            if results is not None:\n                all_results.append(results)\n                \n                print(f\"\\nâœ… Video processing complete!\")\n                print(f\"ğŸ“‚ Results saved:\")\n                print(f\"  ğŸ¥ Output video: {os.path.basename(results['output_video'])}\")\n                print(f\"  ğŸ’¾ NPZ pose data: {os.path.basename(results['npz_file'])}\")\n                print(f\"  âœ¨ Processed {results['processed_frames']} frames\")\n                print()\n            else:\n                print(f\"âŒ Failed to process {video_file}\")\n                \n        except Exception as e:\n            print(f\"ğŸ’¥ Error processing {video_file}: {str(e)}\")\n            continue\n    \n    print(f\"\\nğŸ‰ All videos processed successfully!\")\n    print(f\"ğŸ“ Check the '{BASE_OUTPUT_DIR}' directory for results\")\n    print(f\"\\nğŸ“‹ Quick Access:\")\n    print(f\"  ğŸ¬ Output videos: {output_paths['output_videos']}\")\n    print(f\"  ğŸ’¾ NPZ pose data: {output_paths['pose_data_npz']}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:21:46.830149Z","iopub.execute_input":"2025-08-28T05:21:46.830481Z","iopub.status.idle":"2025-08-28T05:22:08.454282Z","shell.execute_reply.started":"2025-08-28T05:21:46.830457Z","shell.execute_reply":"2025-08-28T05:22:08.453459Z"}},"outputs":[{"name":"stdout","text":"Loading pose estimation model...\nload /root/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\nload /root/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.onnx with onnxruntime backend\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0;93m2025-08-28 05:21:46.912625616 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:21:46.912661929 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n\u001b[0;93m2025-08-28 05:21:46.991684704 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2025-08-28 05:21:46.991710777 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¯ Found 1 video(s) to process:\n  ğŸ“¹ test.mp4\n\nâš™ï¸  Processing settings:\n  ğŸ–¥ï¸  Device: cuda\n  ğŸ”§ Backend: onnxruntime\n  â­ï¸  Frame skip: 1 (process every 1 frame(s))\n  ğŸ”¢ Max frames: unlimited\n\nğŸ“‹ Organizing input videos...\nğŸ“ Copied input video: test.mp4\n\n======================================================================\nğŸš€ Processing: test.mp4\n======================================================================\nğŸ¬ Processing video: test.mp4\nğŸ“¹ Video info: 1920x1080, 25 FPS, 315 frames, 12.60s\nğŸ”„ Processing frames...\nâ³ Progress: 0.0% - Frame 0/315\nâ³ Progress: 39.7% - Frame 125/315\nâ³ Progress: 79.4% - Frame 250/315\nğŸ’¾ Saving pose data to NPZ format...\nğŸ’¾ NPZ file saved: /kaggle/working/pose_estimation_results/pose_data_npz/test_pose_data.npz\nğŸ“ Data shape: 315 frames, up to 2 persons, 17 keypoints each\n\nâœ… Video processing complete!\nğŸ“‚ Results saved:\n  ğŸ¥ Output video: test_pose_estimation.mp4\n  ğŸ’¾ NPZ pose data: test_pose_data.npz\n  âœ¨ Processed 315 frames\n\n\nğŸ‰ All videos processed successfully!\nğŸ“ Check the '/kaggle/working/pose_estimation_results' directory for results\n\nğŸ“‹ Quick Access:\n  ğŸ¬ Output videos: /kaggle/working/pose_estimation_results/output_videos\n  ğŸ’¾ NPZ pose data: /kaggle/working/pose_estimation_results/pose_data_npz\n","output_type":"stream"}],"execution_count":17}]}