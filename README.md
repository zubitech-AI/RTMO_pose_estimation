# Pose Estimation with RTMO

A real-time human pose estimation system using the RTMO (Real-Time Multi-Person Pose Estimation) model. This project processes video input to detect and visualize human poses with high accuracy and performance.

### ğŸ“ Project Structure

RTMO_pose_estimation\
â”œâ”€â”€ src\
â”‚   â”œâ”€â”€ rtmo_pose.ipynb                  \
â”œâ”€â”€ data/\
â”‚   â”œâ”€â”€ test.mp4                        \
â”‚   â””â”€â”€ test_pose_data.npz               \
â”œâ”€â”€ outputs/\
â”‚   â””â”€â”€ test_pose_estimation.mp4         \
â”œâ”€â”€ requirements.txt             \
â”œâ”€â”€ LICENSE                     \
â””â”€â”€ README.md                   

# ğŸš€ Features
+ Real-time multi-person pose estimation

+ High accuracy keypoint detection

+ Efficient video processing pipeline

+ Visualization of pose skeletons and keypoints

+ Export capabilities for processed data

# ğŸ› ï¸ Installation
### Prerequisites
* Python 3.8 or higher

* pip (Python package manager)

# Setup
## 1. Clone the repository:


git clone https://github.com/zubitech-AI/RTMO_pose_estimation.git \
cd RTMO_pose_estimation
## 2. Create and activate a virtual environment (recommended):


### For Windows
+ python -m venv pose-env 
+ pose-env\Scripts\activate
## 3. Install dependencies:
+ pip install -r requirements.txt
# ğŸ“‹ Requirements
The project requires the following Python packages:

+ numpy>=1.21.0
+ opencv-python>=4.5.0
+ tensorflow>=2.8.0
+ matplotlib>=3.5.0
+ jupyter>=1.0.0
+ scikit-learn>=1.0.0
+ tqdm>=4.62.0
#  ğŸ¯ Usage
## Jupyter Notebook
### 1. Launch Jupyter Notebook:

+ jupyter notebook
### 2. Open and run src/rtmo_pose.ipynb

### 3. The notebook will:

- Process data/test.mp4

- Generate pose estimation data

- Create output video in outputs/test_pose_estimation.mp4

- Save processed data to data/test_pose_data.npz

## Command Line Interface
Alternatively, you can use the Python module:


+ python src/pose_estimator.py --input data/test.mp4 --output outputs/result.mp4
Available command line arguments:

+ --input: Path to input video file

+ --output: Path to save output video

+ --model: Path to model file (default: models/rtmo_model.h5)

+ --confidence: Confidence threshold (default: 0.5)

# ğŸ“Š Results
The output includes:

+ Video with visualized pose skeletons

+ Keypoint data stored in NPZ format

+ Performance metrics (FPS, accuracy)

# ğŸ”§ Customization
You can modify the following parameters in the code:

+ Confidence threshold for detection

+ Keypoint connections and visualization colors

+ Output video resolution and quality

+ Processing batch size for optimization

# ğŸ¤ Contributing
We welcome contributions! Please feel free to submit pull requests, report bugs, or suggest new features.

1. Fork the project

2. Create your feature branch (git checkout -b feature/AmazingFeature)

3. Commit your changes (git commit -m 'Add some AmazingFeature')

4. Push to the branch (git push origin feature/AmazingFeature)

5. Open a pull request

# ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

# ğŸ™ Acknowledgments
+ RTMO model authors and contributors

+ OpenCV community for excellent computer vision tools

+ TensorFlow team for deep learning framework

# ğŸ“ Support
Need Help or Want to Work With Me?
For:

âœ… Source code access

âœ… Project setup assistance

âœ… Custom web apps or websites

âœ… Custom mobile applications

âœ… AI projects, chatbot solutions, and more

âœ… College/university assignments and coding tasks

ğŸ‘‰ Contact Me Directly:

ğŸ“± WhatsApp: â€ª+92 308  4737171â€¬\
ğŸ“· Instagram: @ur_zubi\
ğŸ“§ Email: zubitech906@gmail.com

ğŸ’¬ I usually respond within a few hours. Let's build somethingÂ greatÂ together!
